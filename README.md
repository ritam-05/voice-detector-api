
# VOICE DETECTOR API

ğŸ“‚ Data Collection & Dataset Engineering Pipeline

This section describes the complete data preparation workflow used to train the multi-stage AI voice detection system.

---

1ï¸âƒ£ Human Voice Dataset Collection (OpenSLR)

ğŸ¯ Objective

Collect diverse real human speech across:

Genders

Accents

Recording conditions

Noise levels

Studio & non-studio speech

---

Step-by-Step: Downloading from OpenSLR

ğŸ”¹ Step 1: Visit OpenSLR

Open:

https://openslr.org/


---

ğŸ”¹ Step 2: Selected Datasets

The following datasets were used:

Dataset	Description	Purpose

SLR12 (LibriSpeech)	Audiobooks (clean + noisy)	Clean human baseline
SLR17 (Common Voice older dumps)	Crowd-sourced speech	Accent diversity
SLR62 (VoxCeleb subset)	Real-world interview audio	Realistic human speech
SLR100+ noise datasets	Environmental noise	Degradation simulation



---

ğŸ”¹ Step 3: Download Using wget

Example:

wget https://www.openslr.org/resources/12/train-clean-100.tar.gz

Then extract:

tar -xvf train-clean-100.tar.gz


---

ğŸ”¹ Step 4: Preprocessing Pipeline

For each audio file:

1. Convert to mono


2. Resample to 16kHz


3. Normalize amplitude:



x_{norm} = \frac{x}{\max(|x|) + 10^{-9}}

4. Trim silence using energy threshold:



threshold = 0.02 \cdot max\_amplitude

5. Chunk into 4-second windows (50% overlap)




---

2ï¸âƒ£ AI Dataset Generation â€“ Microsoft Azure TTS

ğŸ¯ Objective

Generate high-quality synthetic voices in multiple languages to simulate real-world AI.


---

Languages Used

ğŸ‡ºğŸ‡¸ English

ğŸ‡®ğŸ‡³ Hindi

ğŸ‡«ğŸ‡· French

ğŸ‡ªğŸ‡¸ Spanish

ğŸ‡©ğŸ‡ª German


All using female voices for consistency testing.


---

Step-by-Step: Azure TTS Generation


---

ğŸ”¹ Step 1: Create Azure Account

Go to:

https://portal.azure.com

Create:

Speech Resource

Get API Key

Get Region



---

ğŸ”¹ Step 2: Install Azure SDK

pip install azure-cognitiveservices-speech


---

ğŸ”¹ Step 3: Generate TTS

Example Python script:

import azure.cognitiveservices.speech as speechsdk

speech_config = speechsdk.SpeechConfig(
    subscription="YOUR_KEY",
    region="YOUR_REGION"
)

speech_config.speech_synthesis_voice_name = "en-US-JennyNeural"

audio_config = speechsdk.audio.AudioOutputConfig(filename="output.wav")

synthesizer = speechsdk.SpeechSynthesizer(
    speech_config=speech_config,
    audio_config=audio_config
)

text = "This is a test sentence generated by artificial intelligence."

synthesizer.speak_text_async(text).get()


---

Voices Used

Language	Voice Model

English	en-US-JennyNeural
Hindi	hi-IN-SwaraNeural
French	fr-FR-DeniseNeural
Spanish	es-ES-ElviraNeural
German	de-DE-KatjaNeural



---

ğŸ”¹ Step 4: Text Diversity

Used:

Conversational text

News text

Emotional speech

Numbers

Technical paragraphs


Each file length:

5â€“30 seconds



---

3ï¸âƒ£ Artificial Augmentation of AI Data

To simulate real-world misuse:

Applied Transformations

ğŸ”¹ Noise Injection

x' = x + \epsilon

Where:

\epsilon \sim \mathcal{N}(0, 10^{-4})


---

ğŸ”¹ Compression Simulation

x' = \tanh(0.8x)


---

ğŸ”¹ Downsample & Upsample

x_{low} = Resample(x, 16000 \rightarrow 8000)

x_{final} = Resample(x_{low}, 8000 \rightarrow 16000)


---

ğŸ”¹ Combined Degradation

Noise + Compression + Resampling


---

4ï¸âƒ£ Features Used in Each Model


---

ğŸ”µ LGBM Baseline Model Features

Used handcrafted features:

MFCC (13)

Spectral centroid

Spectral bandwidth

Zero crossing rate

RMS energy

Pitch variance

Repetition cosine similarity

Spectral flatness

SNR estimate


Input vector size â‰ˆ 50â€“80 features


---

ğŸŸ¢ Stage 1 â€“ Wav2Vec2 Detector

Uses:

Self-supervised learned embeddings

Transformer encoder representations

768-dimensional contextual embeddings

Mean pooled features


Classifier:

768 â†’ 256 â†’ 1
ReLU + Dropout

Output:

P(AI) = \sigma(logit)


---

ğŸ”´ Stage 2 â€“ AASIST Backend

Uses:

Frame-level embeddings (T Ã— 768)

Graph Attention Layers

Temporal Attention Pooling

Cross-frame artifact detection

Classifier head


Designed to detect:

Vocoder artifacts

Synthetic smoothness

Temporal inconsistencies

Subtle spectral manipulation



---

5ï¸âƒ£ Hand-Crafted Verification Features

Used for:

Tie-breaking

Explanation

Edge-case handling



---

Breathing Detection

Autocorrelation + spectral centroid < 1000Hz


---

Pitch Variance

F0 = \frac{SR}{lag}

Low variance â†’ AI-like


---

Spectral Flatness

Flatness = \frac{exp(mean(log(P)))}{mean(P)}


---

SNR

SNR = 20 \log_{10}\left(\frac{RMS}{Noise}\right)


---

Repetition Score

Cosine similarity between MFCC chunk vectors:

sim = \frac{A \cdot B}{||A|| ||B||}


---

6ï¸âƒ£ Dataset Summary

Category	Source

Human Clean	LibriSpeech
Human Realistic	VoxCeleb
Human Diverse	OpenSLR datasets
AI Clean	Azure Neural TTS
AI Degraded	Augmented Azure TTS
Noise Samples	OpenSLR noise datasets





=======
>>>>>>> b1f1d3c (utils added)

# 1ï¸âƒ£ Evolution of the System

## ğŸ”¹ Phase 1 â€“ LGBM LoLo Baseline Model

The first version of the system used a **LightGBM (LGBM) classifier** trained on low-level handcrafted acoustic features.

### ğŸ“Š Performance

* Accuracy: **~85%**
* Weakness: Failed on **high-quality studio recordings**
* Training bias: Mostly low-quality, noisy recordings
* Could not generalize to:

  * Clean AI
  * Studio human voices
  * Augmented / compressed audio

This version served as a strong baseline but exposed the need for:

* Better representation learning
* Robustness to degradation
* AI artifact detection

---

# 2ï¸âƒ£ Phase 2 â€“ Hand-Crafted Human / AI Feature Enhancements

To fix baseline weaknesses, we introduced signal-level feature engineering.

### ğŸ§  Human-Degraded Detection

We defined degraded audio using:

#### Signal-to-Noise Ratio (SNR)

[
SNR_{dB} = 20 \log_{10} \left( \frac{RMS}{NoiseFloor} \right)
]

Where:

* ( RMS = \sqrt{\frac{1}{N} \sum x^2} )
* NoiseFloor = 5th percentile amplitude

Degraded if:

* SNR < 18 dB

---

#### Spectral Flatness

[
Flatness = \frac{\exp(\text{mean}(\log(P)))}{\text{mean}(P)}
]

Where:

* (P) = power spectrum

Higher flatness â†’ more noise-like
Lower flatness â†’ tonal/studio

---

#### Clipping Ratio

[
Clipped = \frac{#(|x| > 0.99)}{N}
]

Used to detect distortion.

---

### ğŸ¤– AI Artifact Detection

We engineered AI-specific indicators:

* **Repetition Score**

  * MFCC cosine similarity across chunks
  * High similarity â†’ looped patterns

* **Pitch Variance**

  * Estimated F0 via autocorrelation
  * Low variance â†’ synthetic

* **Vocoder Artifact Score**
  [
  Score = Flatness + 0.02(1 - HF_{ratio})
  ]

* **Dynamics Ratio**
  [
  dyn = \frac{\sigma(RMS)}{\mu(RMS)}
  ]

Low dynamics â†’ monotone AI

---

This improved robustness, but classical ML still lacked deep representation power.

---

# 3ï¸âƒ£ Phase 3 â€“ Stage 1 Deep Model (Wav2Vec2)

We upgraded to a **representation learning model**:

### Architecture

* `facebook/wav2vec2-base`
* Mean pooled embeddings
* MLP head:

  * 768 â†’ 256 â†’ 1

### Stage 1 Role

Binary classifier:

* Outputs probability of AI

### Stage 1 Constraints & Thresholds

```python
S1_HUMAN_RECHECK_THRESHOLD = 0.40
S1_AI_CHECK_THRESHOLD = 0.75
S1_VERY_CONFIDENT = 0.90
S1_CONFIDENT = 0.82
```

### Decision Logic

| Range     | Meaning                     |
| --------- | --------------------------- |
| < 0.40    | HUMAN (trusted immediately) |
| 0.40â€“0.75 | Ambiguous                   |
| > 0.75    | AI candidate                |
| > 0.90    | Very confident AI           |

Stage 1 became strong â€” but occasionally overconfident.

So we built Stage 2.

---

# 4ï¸âƒ£ Stage 2 â€“ AASIST Advanced Verifier

## What is AASIST?

AASIST is a **graph attention-based backend** that analyzes:

* Frame-level temporal patterns
* Spectral artifacts
* Subtle vocoder cues
* Cross-frame dependencies

Architecture:

```
Wav2Vec2 Encoder â†’ AASIST Backend â†’ Classifier
```

---

# ğŸ§  Core Innovation

## Confidence-Weighted Adaptive Verification

Instead of hard thresholds:

> Different confidence levels require different strategies.

---

# ğŸ¯ Decision Flow

---

## Case 1: Stage 1 says HUMAN (< 0.40)

âœ” Trust immediately
âœ” No AASIST verification
âœ” Minimizes false AI labeling

---

## Case 2: Stage 1 says AI (> 0.75)

Tiered adaptive logic:

---

###  TIER 1: Very Confident AI (> 0.90)

| AASIST    | Decision                 |
| --------- | ------------------------ |
| â‰¥ 0.40    | AI (70% S1 + 30% AASIST) |
| 0.20â€“0.40 | AI (trust S1)            |
| < 0.20    | HUMAN                    |

---

###  TIER 2: Confident AI (0.82â€“0.90)

| AASIST    | Decision                |
| --------- | ----------------------- |
| â‰¥ 0.45    | AI (65/35 weighted)     |
| 0.25â€“0.45 | Weighted check          |
| < 0.25    | Feature-based tie break |

---

###  TIER 3: Moderate AI (0.75â€“0.82)

| AASIST    | Decision     |
| --------- | ------------ |
| â‰¥ 0.50    | AI (50/50)   |
| 0.30â€“0.50 | INCONCLUSIVE |
| < 0.30    | HUMAN        |

---

## Case 3: Stage 1 Ambiguous (0.40â€“0.75)

Trust AASIST more:

[
FinalScore = 0.4 \cdot S1 + 0.6 \cdot AASIST
]

| AASIST    | Decision          |
| --------- | ----------------- |
| â‰¥ 0.55    | AI                |
| < 0.35    | HUMAN             |
| 0.35â€“0.55 | Weighted decision |

---

# ğŸ“Š Why This Is Powerful

### âœ… Reduces False Positives

Human voices rarely cross S1 < 0.40.

### âœ… Reduces False Negatives

Very confident S1 AI (>0.90) no longer blocked by mild AASIST uncertainty.

### âœ… Handles Studio Audio

Hand-crafted features compensate for over-clean recordings.

### âœ… Handles Augmented AI

Vocoder + repetition detection catches artifacts.

---

# ğŸ“ˆ Example

Example:

```
S1 = 0.85
AASIST = 0.38
```

Weighted:

[
0.6(0.85) + 0.4(0.38) = 0.662
]

â†’ AI

Old system â†’ INCONCLUSIVE
New system â†’ Correct AI

---

# ğŸ— Architecture Overview

```
Audio Input
    â†“
Chunking (4s, 50% overlap)
    â†“
Stage 1 (Wav2Vec2 + MLP)
    â†“
Hand-crafted feature extraction
    â†“
AASIST (conditional verification)
    â†“
Confidence-weighted decision fusion
    â†“
Final Label + Explanation
```

---

# ğŸ” Explanation Engine

The system produces structured layman explanations based on:

* Breathing detection
* Pitch variance
* Dynamics ratio
* Repetition score
* Vocoder artifacts
* Clipping
* Reverb
* Micro-noises

This makes the system explainable â€” critical for hackathon judging.

---

# ğŸš€ Final System Characteristics

| Feature           | Supported    |
| ----------------- | ------------ |
| Studio Human      | âœ…            |
| Low Quality Human | âœ…            |
| Augmented AI      | âœ…            |
| Clean AI          | âœ…            |
| Edge Cases        | INCONCLUSIVE |
| Confidence Output | Yes          |
| Explainability    | Yes          |

---

It is a **multi-stage adaptive AI verification system** built to handle real-world edge cases.

